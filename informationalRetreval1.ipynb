{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1a_xEBCm0KuNCm5SrZqU7TgILryBVTc6s",
      "authorship_tag": "ABX9TyN2P+yWsnfbuRRBrUYU6W+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SkShoheb33/Information-Retreval/blob/main/informationalRetreval1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GewLcHyQTdH5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upIwowsqmocw",
        "outputId": "5eaf0719-4ec9-468f-b325-e0ba89b018ff"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.internals import blocks\n",
        "class InfoRetrival:\n",
        "  def __init__(self,corpus):  \n",
        "    self.corpus = corpus\n",
        "    self.documents = []\n",
        "    self.titles = []\n",
        "    self.tokens = []\n",
        "    self.termDocs = {}\n",
        "    self.invertIndex = {}\n",
        "    self.boolIndex = {}\n",
        "    self.positionIndex = {}\n",
        "    self.biwords = {}\n",
        "    self.blocks = {}\n",
        "    self.BSBI = {}\n",
        "    self.SPIMI = {}\n",
        "    for i in os.listdir(corpus):\n",
        "      if i.split('.')[-1] == 'txt':\n",
        "        self.documents.append(open(corpus+i).read())\n",
        "        self.titles.append(i[:-4])\n",
        "        # print(self.titles[-1])\n",
        "\n",
        "  # def removeDuplicates(self,l)\n",
        "\n",
        "\n",
        "  def makeTerms(self,document):\n",
        "    terms = document.split(\" \")\n",
        "    n = len(terms)\n",
        "    for i in range(n):\n",
        "      if not terms[i].islower():\n",
        "        terms[i] = terms[i].lower()\n",
        "      terms[i] = re.sub(r'[^a-zA-Z0-9]', '', terms[i])\n",
        "    return terms\n",
        "\n",
        "\n",
        "\n",
        "  def removeStopWords(self,terms):\n",
        "    # \"i\",\"am\",\"it\",\"is\",\"the\",\"we\",\"was\",\"a\",\"an\",\"and\",\"or\",\"to\",\"so\",\n",
        "    stopWords = [\".\",\";\",\"!\",\"@\",\"#\",\"$\",\"%\",\"^\",\"&\",\"*\",\"(\",\")\",\"{\",\"}\",\"[\",\"]\",\":\",\"'\",\"/\",\"<\",\">\",'\"',\"+\"]\n",
        "    i = 0\n",
        "    while i < len(terms):\n",
        "      if terms[i] in stopWords  or len(terms[i])==0:\n",
        "        terms.remove(terms[i])\n",
        "      else:\n",
        "        i+=1\n",
        "    return terms\n",
        "\n",
        "\n",
        "  def stemming(self):\n",
        "    ps = PorterStemmer()\n",
        "    for title in self.termDocs:\n",
        "      n = len(self.termDocs[title])\n",
        "      for i in range(n):\n",
        "        self.termDocs[title][i] = ps.stem(self.termDocs[title][i])\n",
        "\n",
        "  def termsDocuments(self):\n",
        "    for title,docs in zip(self.titles,self.documents):\n",
        "      self.termDocs[title] = self.removeStopWords(self.makeTerms(docs))\n",
        "\n",
        "\n",
        "\n",
        "  def tokenize(self):\n",
        "    self.termsDocuments()\n",
        "    self.stemming()\n",
        "    for title in self.termDocs:\n",
        "      n = len(self.termDocs[title])\n",
        "      for i in range(n):\n",
        "        if self.termDocs[title][i] in self.tokens:\n",
        "          continue\n",
        "        self.tokens.append(self.termDocs[title][i])\n",
        "    self.tokens.sort()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def invertedIndex(self):\n",
        "    self.tokenize()\n",
        "    for token in self.tokens:\n",
        "      for doc in self.termDocs:\n",
        "        if token in self.termDocs[doc]:\n",
        "          if token not in self.invertIndex:\n",
        "            self.invertIndex[token] = []\n",
        "          self.invertIndex[token].append(doc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def booleanIndex(self):\n",
        "    self.tokenize()\n",
        "    for i in self.tokens:\n",
        "      for j in self.titles:\n",
        "        if i not in self.boolIndex:\n",
        "          self.boolIndex[i] = []\n",
        "        if j in self.invertIndex[i]:\n",
        "          self.boolIndex[i].append(1)\n",
        "        else:\n",
        "          self.boolIndex[i].append(0)\n",
        "\n",
        "\n",
        "          \n",
        "  def positionalIndex(self):\n",
        "    ps = PorterStemmer()\n",
        "    for i in self.tokens:\n",
        "      self.positionIndex[i] = {}\n",
        "      for j in self.titles:\n",
        "        document = open(self.corpus+(j+\".txt\")).read().lower().split(\" \")\n",
        "        n = len(document)\n",
        "        row = []\n",
        "        for k in range(n):\n",
        "          if i == ps.stem(document[k]):\n",
        "            row.append(k)\n",
        "        if len(row)>0:\n",
        "          self.positionIndex[i][j] = row\n",
        "\n",
        "\n",
        "\n",
        "  def biwordIndexing(self):\n",
        "    for token in self.tokens:\n",
        "      n = len(token)\n",
        "      if n>0:\n",
        "        l = ['$'+token[0]]\n",
        "        if ('$'+token[0]) not in self.biwords :\n",
        "          self.biwords['$'+token[0]] = [token]\n",
        "        else:\n",
        "          self.biwords['$'+token[0]].append(token)\n",
        "        for i in range(n-1):\n",
        "          if (token[i:i+2]) not in self.biwords :\n",
        "            self.biwords[token[i:i+2]] = [token]\n",
        "          else:\n",
        "            self.biwords[token[i:i+2]].append(token)\n",
        "        if (token[n-1]+'$') not in self.biwords :\n",
        "          self.biwords[token[n-1]+'$'] = [token]\n",
        "        else:\n",
        "          self.biwords[token[n-1]+'$'].append(token)\n",
        "\n",
        "  def wildcardQuery(self,query):\n",
        "    s = ['$'+query[0]]\n",
        "    k = 0\n",
        "    if s[-1] in [\"*$\",\"$*\"]:\n",
        "      s = ['$'+query[1]]\n",
        "      k = 1\n",
        "    for i in range(k,len(query)-1):\n",
        "      if '*' not in query[i:i+2]:\n",
        "        s.append(query[i:i+2])\n",
        "    s.append(query[-1]+'$')\n",
        "    result = set(self.biwords[s[0]])\n",
        "    for i in range(1,len(s)):\n",
        "      result = result.intersection(set(self.biwords[s[i]]))\n",
        "    return list(result)\n",
        "\n",
        "\n",
        "  def printBooleanIndex(self):\n",
        "    return pd.DataFrame(self.boolIndex,index=self.titles)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def boolean_and(self,row1,row2):\n",
        "    n = len(row1)\n",
        "    for i in range(n):\n",
        "      if row1[i] == row2[i] == 1:\n",
        "        row1[i] = 1\n",
        "      else:\n",
        "        row1[i] = 0\n",
        "    return row1\n",
        "\n",
        "\n",
        "\n",
        "  def boolean_or(self,row1,row2):\n",
        "    n = len(row1)\n",
        "    for i in range(n):\n",
        "      if row1[i] == 1 or row2[i] == 1:\n",
        "        row1[i] = 1\n",
        "      else:\n",
        "        row1[i] = 0\n",
        "    return row1\n",
        "\n",
        "\n",
        "\n",
        "  def boolean_not(self,row):\n",
        "    n = len(row)\n",
        "    for i in range(n):\n",
        "      if row[i] :\n",
        "        row[i] = 0\n",
        "      else:\n",
        "        row[i] = 1\n",
        "    return row\n",
        "\n",
        "\n",
        "\n",
        "  def booleanQuery(self,query):\n",
        "    query = query.split(\" \")\n",
        "    opearator = \"\"\n",
        "    flag = False\n",
        "    n = 10\n",
        "    result = []\n",
        "    for i in query:\n",
        "      if i in ['and','or']:\n",
        "        opearator = i\n",
        "      elif i == 'not':\n",
        "        flag = True\n",
        "      else:\n",
        "        if flag :\n",
        "          flag = False\n",
        "          result = self.boolean_not(self.boolIndex[i])\n",
        "        if opearator == \"and\":\n",
        "          result = self.boolean_and(result,self.boolIndex[i])\n",
        "          operator = \"\"\n",
        "        elif opearator == \"or\":\n",
        "          result = self.boolean_or(result,self.boolIndex[i])\n",
        "          operator = \"\"\n",
        "        else:\n",
        "          result = self.boolIndex[i]\n",
        "    ans = []\n",
        "    for i in range(n):\n",
        "      if result[i]:\n",
        "        ans.append(self.titles[i])\n",
        "    return ans\n",
        "\n",
        "\n",
        "  def commonDocument(self,terms):\n",
        "    commonDocs = set(self.positionIndex[terms[0]])\n",
        "    for i in range(1,len(terms)):\n",
        "      commonDocs = commonDocs.intersection(self.positionIndex[terms[i]])\n",
        "    return list(commonDocs)\n",
        "\n",
        "\n",
        "  def phraseQuery(self,query):\n",
        "    print(\"HELLo\")\n",
        "    positions = {}\n",
        "    ps = PorterStemmer()\n",
        "    query = query.lower().split(\" \")\n",
        "    n = len(query)\n",
        "    for i in range(n):\n",
        "      query[i] = ps.stem(query[i])\n",
        "    result = []\n",
        "    commonDocs = self.commonDocument(query)\n",
        "    for doc in commonDocs:\n",
        "      for position in self.positionIndex[query[0]][doc]:\n",
        "        i = 1\n",
        "        while i<n:\n",
        "          if (position+i) in self.positionIndex[query[i]][doc]:\n",
        "            i+=1\n",
        "          else:\n",
        "            break\n",
        "        if i == n:\n",
        "          result.append(doc)\n",
        "    return result\n",
        "\n",
        "\n",
        "  def correctWords(self,word):\n",
        "    result = {}\n",
        "    row = list(word)\n",
        "    for token in self.tokens:\n",
        "      dp = []\n",
        "      column = list(token)\n",
        "      for i in range(len(row)+1):\n",
        "        dp.append([0 for j in range(len(column)+1)])\n",
        "      for i in range(len(row)+1):\n",
        "        for j in range(len(column)+1):\n",
        "          if j == 0:\n",
        "            dp[i][j] = i\n",
        "          elif i == 0:\n",
        "            dp[i][j] = j\n",
        "      for i in range(1,len(row)+1):\n",
        "        for j in range(1,len(column)+1):\n",
        "          if row[i-1] == column[j-1]:\n",
        "            dp[i][j] = min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])\n",
        "          else:\n",
        "            dp[i][j] = min(dp[i-1][j],dp[i][j-1],dp[i-1][j-1])+1\n",
        "      # if dp[-1][-1] in [1,2]:\n",
        "      if dp[-1][-1] not in result:\n",
        "        result[dp[-1][-1]] = []\n",
        "      result[dp[-1][-1]].append(token)\n",
        "      # print(dp[-1][-1])\n",
        "    return result[min(result.keys())]\n",
        "\n",
        "\n",
        "  def leastFactor(self,n):\n",
        "    for i in range(2,n):\n",
        "      if n%i == 0:\n",
        "        return i\n",
        "    return 1            \n",
        "\n",
        "\n",
        "  def makeBlock(self):\n",
        "    ps = PorterStemmer()\n",
        "    n = len(self.titles)\n",
        "    b = self.leastFactor(n)\n",
        "    c = 0\n",
        "    self.blocks = {}\n",
        "    c = 0\n",
        "    # print(b,n)\n",
        "    for i in range(0,n,b):\n",
        "      self.blocks[\"block\"+str(c+1)] = {}\n",
        "      for j in range(i,i+b):\n",
        "        terms = self.removeStopWords(self.makeTerms(open(self.corpus+self.titles[j]+\".txt\").read()))\n",
        "        for term in terms:\n",
        "          term = ps.stem(term)\n",
        "          if term not in self.blocks[\"block\"+str(c+1)]:\n",
        "            self.blocks[\"block\"+str(c+1)][term] = []\n",
        "          if self.titles[j] not in self.blocks[\"block\"+str(c+1)][term]:\n",
        "            self.blocks[\"block\"+str(c+1)][term].append(self.titles[j])\n",
        "      c+=1\n",
        "\n",
        "\n",
        "\n",
        "  def mergeDocs(self,doc1,doc2):\n",
        "    doc1.sort()\n",
        "    doc2.sort()\n",
        "    m = len(doc1)\n",
        "    n = len(doc2)\n",
        "    result = []\n",
        "    i,j = 0,0\n",
        "    while i<m and j<n:\n",
        "      if doc1[i] == doc2[j]:\n",
        "        result.append(doc1[i])\n",
        "        i+=1\n",
        "        j+=1\n",
        "      elif doc1[i] < doc2[j]:\n",
        "        result.append(doc1[i])\n",
        "        i+=1\n",
        "      else:\n",
        "        result.append(doc2[j])\n",
        "        j+=1\n",
        "    while i < m:\n",
        "      result.append(doc1[i])\n",
        "      i+=1\n",
        "    while j < n:\n",
        "      result.append(doc2[j])\n",
        "      j+=1\n",
        "    return result\n",
        "\n",
        "\n",
        "\n",
        "  def mergeBlocks(self,blockA,blockB):\n",
        "    termsA = list(blockA.keys())\n",
        "    termsB = list(blockB.keys())\n",
        "    termsA.sort()\n",
        "    termsB.sort()\n",
        "    m = len(termsA)\n",
        "    n = len(termsB)\n",
        "    result = {}\n",
        "    i,j = 0,0\n",
        "    while i < m  and j <n :\n",
        "      if termsA[i] == termsB[j]:\n",
        "        result[termsA[i]] = self.mergeDocs(blockA[termsA[i]],blockB[termsB[j]])\n",
        "        i+=1\n",
        "        j+=1\n",
        "      elif termsA[i] < termsB[j]:\n",
        "        result[termsA[i]] =blockA[termsA[i]]\n",
        "        i+=1\n",
        "      else:\n",
        "        result[termsB[j]] = blockB[termsB[j]]\n",
        "        j+=1\n",
        "    while  i < m:\n",
        "      result[termsA[i]] = blockA[termsA[i]]\n",
        "      i+=1\n",
        "    while  j < n:\n",
        "      result[termsB[j]] = blockB[termsB[j]]\n",
        "      j+=1\n",
        "    return result\n",
        "\n",
        "\n",
        "  def blockSortBasedIndexing(self):\n",
        "    n = len(self.blocks)\n",
        "    self.BSBI = self.mergeBlocks(self.blocks['block1'],{})\n",
        "    for i in range(2,n+1):\n",
        "      self.BSBI = self.mergeBlocks(self.BSBI,self.blocks[f'block{i}'])\n",
        "\n",
        "\n",
        "  def singlePassInMemoryIndexing(self,corpus):\n",
        "    path = self.corpus+'/dictionary.json'\n",
        "    if os.path.exists(path):\n",
        "      with open(path,'r') as f:\n",
        "        self.SPIMI = json.load(f)\n",
        "    for title in self.titles:\n",
        "      if len(self.SPIMI)==0:\n",
        "        for word in sorted(list(set(self.termDocs[title]))):\n",
        "          self.SPIMI[word] = [title]\n",
        "      else:\n",
        "        for word in sorted(list(set(self.termDocs[title]))):\n",
        "          if word in self.SPIMI:\n",
        "            self.SPIMI[word].append(title)\n",
        "          else:\n",
        "            self.SPIMI[word] = [title]\n",
        "    with open(path,'w') as f:\n",
        "      json.dump(self.SPIMI,f)\n",
        "    # return self.SPIMI\n",
        "\n",
        "      \n",
        "      \n"
      ],
      "metadata": {
        "id": "k88w8C_2UIPQ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "obj = InfoRetrival(\"/content/drive/MyDrive/stories/\")\n",
        "obj.invertedIndex()\n",
        "obj.invertIndex\n",
        "obj.booleanIndex()\n",
        "obj.boolIndex\n",
        "obj.printBooleanIndex()\n",
        "obj.booleanQuery(\"tiger or tiger\")\n",
        "# obj.positionalIndex()\n",
        "# obj.positionIndex\n",
        "# obj.phraseQuery(\"li\")\n",
        "obj.biwordIndexing()\n",
        "obj.biwords\n",
        "obj.wildcardQuery(\"e*e\")\n",
        "obj.correctWords('lon')\n",
        "obj.makeBlock()\n",
        "obj.blocks\n",
        "obj.blockSortBasedIndexing()\n",
        "obj.BSBI\n",
        "obj.singlePassInMemoryIndexing(\"/content/drive/MyDrive/stories\")"
      ],
      "metadata": {
        "id": "DCjEnGpuVYuu"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# obj.printBooleanIndex()"
      ],
      "metadata": {
        "id": "qg_BRT9UKadT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.listdir('/content/drive/MyDrive/stories')\n",
        "# sorted(obj.termDocs['doc1'])\n",
        "# obj.SPIMI"
      ],
      "metadata": {
        "id": "pnX0z7rmieer",
        "outputId": "6a8a8165-4a89-4d2a-c29e-be9fde7a2829",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['doc1.txt',\n",
              " 'doc2.txt',\n",
              " 'doc3.txt',\n",
              " 'doc4.txt',\n",
              " 'doc7.txt',\n",
              " 'doc8.txt',\n",
              " 'doc9.txt',\n",
              " 'doc10.txt',\n",
              " 'doc5.txt',\n",
              " 'doc6.txt',\n",
              " 'dictionary.json']"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    }
  ]
}